#!/bin/bash

###############################################
#         GNU MPI job script example          #
###############################################

#SBATCH --account=ACCOUNT                                   # (-A) Account/project number
#SBATCH --job-name=JOB_NAME                                 # (-J) Job name
#SBATCH --partition=ctest                                   # (-p) Specific slurm partition
#SBATCH --ntasks=8                                          # (-n) Number of total MPI tasks (i.e. processes)
#SBATCH --nodes=2                                           # (-N) Maximum number of nodes to be allocated
#SBATCH --ntasks-per-node=4                                 # Maximum number of tasks on each node
#SBATCH --cpus-per-task=14                                  # (-c) Number of cores per MPI task
#SBATCH --mem=162400M                                       # Memory limit per compute node for the job. Do not use with mem-per-cpu flag.
#SBATCH --time=00:30:00                                     # (-t) Wall time limit (days-hrs:min:sec)
##SBATCH -o log
##SBATCH -e job.%j.err
##SBATCH --mail-type=BEGIN,END,FAIL                         # Mail events (NONE, BEGIN, END, FAIL, ALL)
##SBATCH --mail-user=EMAIL_ADDRESS                          # Where to send mail.  Set this to your email address
#SBATCH --exclude=cpn[3001-3120,3241-3360]                  # Exclude large-memory nodes

LOG_FILE=log

module purge
module use /home/d07222009/module_CALAB
module load gcc/13.2.0 gnu_13.2.0/fftw/3.3.10 gnu_13.2.0/gsl/2.8.0 gnu_13.2.0/hdf5/1.14.4 gnu_13.2.0/openmpi/5.0.5  gnu_13.2.0/openucx/1.18.0
module list 1>>$LOG_FILE 2>&1

# See: https://docs.open-mpi.org/en/v5.0.x/man-openmpi/man1/mpirun.1.html#the-map-by-option
# There are 2 NUMA nodes on each node, 1 per socket
mpirun -map-by ppr:2:numa:pe=14 --report-bindings ./gamer 1>>$LOG_FILE 2>&1
echo "=============================================================" 1>>$LOG_FILE 2>&1
